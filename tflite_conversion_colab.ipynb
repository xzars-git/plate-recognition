{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53068837",
   "metadata": {},
   "source": [
    "# üöÄ YOLOv11 Model Conversion to TFLite\n",
    "\n",
    "Convert **best.pt (Epoch170)** to TFLite format for mobile deployment in Flutter.\n",
    "\n",
    "**Why Google Colab?**\n",
    "- TFLite conversion requires specific dependencies (onnx2tf, tf_keras)\n",
    "- Dependency conflicts on Windows environment\n",
    "- Colab has all required packages pre-installed\n",
    "\n",
    "**Expected Output:**\n",
    "- `best.onnx` - ONNX format (10-11 MB)\n",
    "- `best_fp16.tflite` - FP16 quantized (~10 MB)\n",
    "- `best_int8.tflite` - INT8 quantized (~4-6 MB, fastest on mobile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d23690",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e95cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ultralytics YOLO\n",
    "!pip install -q ultralytics\n",
    "\n",
    "# Install TFLite conversion dependencies\n",
    "!pip install -q tensorflow==2.16.2\n",
    "!pip install -q onnx>=1.12.0\n",
    "!pip install -q onnxsim>=0.4.1\n",
    "!pip install -q onnxruntime>=1.16.0\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")\n",
    "print(\"   TensorFlow: 2.16.2\")\n",
    "print(\"   ONNX: >=1.12.0\")\n",
    "print(\"   ONNX Runtime: >=1.16.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a1da42",
   "metadata": {},
   "source": [
    "## üì§ Step 2: Upload Model File\n",
    "\n",
    "Upload `best.pt` from your local machine to Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a8d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üì§ Please upload best.pt file...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify upload\n",
    "if 'best.pt' in uploaded:\n",
    "    file_size = os.path.getsize('best.pt') / (1024 * 1024)\n",
    "    print(f\"\\n‚úÖ best.pt uploaded successfully!\")\n",
    "    print(f\"   Size: {file_size:.2f} MB\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Error: best.pt not found!\")\n",
    "    print(\"   Please upload the correct file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8673d",
   "metadata": {},
   "source": [
    "## üîç Step 3: Verify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d765e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load model\n",
    "print(\"üì• Loading model...\")\n",
    "model = YOLO('best.pt')\n",
    "\n",
    "# Display model info\n",
    "print(\"\\n‚úÖ Model loaded successfully!\")\n",
    "print(\"\\nüìä Model Information:\")\n",
    "model.info()\n",
    "\n",
    "print(\"\\nüéØ Expected Performance:\")\n",
    "print(\"   Precision: 81.64%\")\n",
    "print(\"   mAP50: 49.14%\")\n",
    "print(\"   Speed: 1.30ms (PyTorch on RTX 3080 Ti)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b48f68",
   "metadata": {},
   "source": [
    "## üîÑ Step 4: Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301320a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"üîÑ Exporting to ONNX format...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = model.export(\n",
    "    format='onnx',\n",
    "    imgsz=640,\n",
    "    simplify=True,\n",
    "    opset=12,\n",
    "    dynamic=False\n",
    ")\n",
    "\n",
    "export_time = time.time() - start_time\n",
    "onnx_size = os.path.getsize(onnx_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n‚úÖ ONNX export complete!\")\n",
    "print(f\"   File: {onnx_path}\")\n",
    "print(f\"   Size: {onnx_size:.2f} MB\")\n",
    "print(f\"   Export time: {export_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f08c47",
   "metadata": {},
   "source": [
    "## üì± Step 5: Export to TFLite (FP16 Quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bcadbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì± Exporting to TFLite (FP16)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Export to TFLite with FP16 quantization\n",
    "tflite_fp16_path = model.export(\n",
    "    format='tflite',\n",
    "    imgsz=640,\n",
    "    int8=False,\n",
    "    half=True  # FP16 quantization\n",
    ")\n",
    "\n",
    "export_time = time.time() - start_time\n",
    "tflite_size = os.path.getsize(tflite_fp16_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n‚úÖ TFLite FP16 export complete!\")\n",
    "print(f\"   File: {tflite_fp16_path}\")\n",
    "print(f\"   Size: {tflite_size:.2f} MB\")\n",
    "print(f\"   Export time: {export_time:.1f}s\")\n",
    "print(f\"\\nüìä Quantization: FP16 (half precision)\")\n",
    "print(f\"   Expected performance: ~81% accuracy, 20-30ms mobile inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec24980c",
   "metadata": {},
   "source": [
    "## üöÄ Step 6: Export to TFLite (INT8 Quantization - Fastest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c9082",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Exporting to TFLite (INT8)...\")\n",
    "print(\"‚ö†Ô∏è  This will take longer (~2-5 minutes)\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Export to TFLite with INT8 quantization\n",
    "tflite_int8_path = model.export(\n",
    "    format='tflite',\n",
    "    imgsz=640,\n",
    "    int8=True,  # INT8 quantization\n",
    "    data='coco128.yaml'  # Use sample dataset for calibration\n",
    ")\n",
    "\n",
    "export_time = time.time() - start_time\n",
    "tflite_int8_size = os.path.getsize(tflite_int8_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n‚úÖ TFLite INT8 export complete!\")\n",
    "print(f\"   File: {tflite_int8_path}\")\n",
    "print(f\"   Size: {tflite_int8_size:.2f} MB\")\n",
    "print(f\"   Export time: {export_time:.1f}s\")\n",
    "print(f\"\\nüìä Quantization: INT8 (integer precision)\")\n",
    "print(f\"   Expected performance: ~80% accuracy, 10-20ms mobile inference\")\n",
    "print(f\"   Size reduction: {((tflite_size - tflite_int8_size) / tflite_size * 100):.1f}% smaller than FP16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713343c",
   "metadata": {},
   "source": [
    "## üìä Step 7: Export Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "summary_data = {\n",
    "    'Format': ['PyTorch', 'ONNX', 'TFLite FP16', 'TFLite INT8'],\n",
    "    'Filename': ['best.pt', onnx_path, tflite_fp16_path, tflite_int8_path],\n",
    "    'Size (MB)': [\n",
    "        f\"{os.path.getsize('best.pt') / (1024 * 1024):.2f}\",\n",
    "        f\"{onnx_size:.2f}\",\n",
    "        f\"{tflite_size:.2f}\",\n",
    "        f\"{tflite_int8_size:.2f}\"\n",
    "    ],\n",
    "    'Platform': ['Desktop', 'Mobile/Server', 'Mobile', 'Mobile (Optimized)'],\n",
    "    'Est. Speed': ['1.30ms', '20-50ms', '20-30ms', '10-20ms'],\n",
    "    'Accuracy': ['81.64%', '81.64%', '~81%', '~80%']\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä EXPORT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüéØ Recommendations:\")\n",
    "print(\"   ‚Ä¢ Use ONNX for server/desktop deployment (best accuracy)\")\n",
    "print(\"   ‚Ä¢ Use TFLite FP16 for mobile (good balance)\")\n",
    "print(\"   ‚Ä¢ Use TFLite INT8 for mobile (fastest, smallest)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c3f45",
   "metadata": {},
   "source": [
    "## üì• Step 8: Download Converted Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f5448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üì• Download converted models:\")\n",
    "print(\"\\n1Ô∏è‚É£ ONNX format (recommended for server):\")\n",
    "files.download(onnx_path)\n",
    "print(f\"   ‚úÖ Downloaded: {onnx_path}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ TFLite FP16 (good for mobile):\")\n",
    "files.download(tflite_fp16_path)\n",
    "print(f\"   ‚úÖ Downloaded: {tflite_fp16_path}\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ TFLite INT8 (fastest for mobile):\")\n",
    "files.download(tflite_int8_path)\n",
    "print(f\"   ‚úÖ Downloaded: {tflite_int8_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ All models downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4867b9",
   "metadata": {},
   "source": [
    "## üß™ Step 9: Test Inference (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample image\n",
    "print(\"üß™ Testing inference on sample image...\")\n",
    "\n",
    "# Upload test image\n",
    "print(\"\\nüì§ Upload a test image (optional):\")\n",
    "test_upload = files.upload()\n",
    "\n",
    "if test_upload:\n",
    "    test_image = list(test_upload.keys())[0]\n",
    "    \n",
    "    # Run inference with PyTorch model\n",
    "    print(f\"\\nüîç Running inference on {test_image}...\")\n",
    "    results = model.predict(test_image, conf=0.25, verbose=False)\n",
    "    \n",
    "    # Show results\n",
    "    print(f\"\\n‚úÖ Detection complete!\")\n",
    "    print(f\"   Plates detected: {len(results[0].boxes)}\")\n",
    "    \n",
    "    for i, box in enumerate(results[0].boxes, 1):\n",
    "        conf = float(box.conf[0])\n",
    "        print(f\"   Plate {i}: {conf:.2%} confidence\")\n",
    "    \n",
    "    # Show annotated image\n",
    "    annotated = results[0].plot()\n",
    "    from IPython.display import Image, display\n",
    "    import cv2\n",
    "    cv2.imwrite('result.jpg', annotated)\n",
    "    display(Image('result.jpg'))\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  No test image uploaded, skipping inference test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c23209",
   "metadata": {},
   "source": [
    "## üì± Flutter Integration Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e0398",
   "metadata": {},
   "source": [
    "### Option 1: TFLite Flutter Plugin\n",
    "\n",
    "```yaml\n",
    "# pubspec.yaml\n",
    "dependencies:\n",
    "  tflite_flutter: ^0.10.0\n",
    "  image: ^4.0.0\n",
    "```\n",
    "\n",
    "```dart\n",
    "// main.dart\n",
    "import 'package:tflite_flutter/tflite_flutter.dart';\n",
    "\n",
    "class PlateDetector {\n",
    "  late Interpreter interpreter;\n",
    "  \n",
    "  Future<void> loadModel() async {\n",
    "    interpreter = await Interpreter.fromAsset('assets/best_int8.tflite');\n",
    "    print('Model loaded: ${interpreter.getInputTensors()}');\n",
    "  }\n",
    "  \n",
    "  Future<List<Detection>> detectPlate(Image image) async {\n",
    "    // Preprocess: resize to 640x640\n",
    "    var input = preprocessImage(image);\n",
    "    \n",
    "    // Run inference\n",
    "    var output = List.filled(1 * 5 * 8400, 0.0).reshape([1, 5, 8400]);\n",
    "    interpreter.run(input, output);\n",
    "    \n",
    "    // Post-process\n",
    "    return parseDetections(output);\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Option 2: ONNX Runtime Flutter\n",
    "\n",
    "```yaml\n",
    "# pubspec.yaml\n",
    "dependencies:\n",
    "  onnxruntime: ^1.15.0\n",
    "```\n",
    "\n",
    "```dart\n",
    "import 'package:onnxruntime/onnxruntime.dart';\n",
    "\n",
    "final session = OrtSession.fromAsset('assets/best.onnx');\n",
    "final output = session.run([input]);\n",
    "```\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **Use INT8 model** for fastest inference (10-20ms)\n",
    "2. **Enable GPU delegate** if device supports\n",
    "3. **Pre-allocate buffers** for inference\n",
    "4. **Batch processing** for multiple images\n",
    "5. **Rotation correction** before inference (use same algorithm from Python)\n",
    "\n",
    "---\n",
    "\n",
    "**‚úÖ Conversion complete! Your models are ready for mobile deployment.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
